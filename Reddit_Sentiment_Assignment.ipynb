{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This handy piece of code changes Jupyter Notebooks margins to fit your screen.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be sure you've installed the praw and tqdm libraries. If you haven't you can run the line below.  Node.js in required to install the jupyter widgets in a few cells.  These two cells can take a while to run and won't show progress; you can also run the commands in the command prompt (without the !) to see the progress as it installs.\n",
    "\n",
    "If conda is taking a long time, you might try the mamba installer: https://github.com/TheSnakePit/mamba\n",
    "`conda install -c conda-forge mamba -y`\n",
    "Then installing packages with mamba should be done from the command line (console or terminal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install tqdm praw nodejs -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the jupyter widget to enable tqdm to work with jupyter lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Reddit Comments for a Sentiment Analysis - Assignment\n",
    "### Go through the notebook and complete the code where prompted\n",
    "##### This assignment was adapted from a number of sources including: http://www.storybench.org/how-to-scrape-reddit-with-python/ and https://towardsdatascience.com/scraping-reddit-data-1c0af3040768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import praw # Import the Praw library: https://praw.readthedocs.io/en/latest/code_overview/reddit_instance.html\n",
    "import pandas as pd # Import Pandas library: https://pandas.pydata.org/\n",
    "import datetime as dt # Import datetime library\n",
    "import matplotlib.pyplot as plt # Import Matplot lib for plotting\n",
    "from tqdm.notebook import tqdm  # progress bar used in loops\n",
    "\n",
    "import credentials as cred  # make sure to enter your API credentials in the credentials.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "### In the cell below, enter your client ID, client secret, user agent, username, and password in the appropitate place withing the quotation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Praw (Python Reddit API Wrapper) is used to communicate with Reddit\n",
    "reddit = praw.Reddit(client_id='',\n",
    "                     client_secret='',\n",
    "                     user_agent='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "## In the cell below, enter a subreddit you which to compare the sentiment of the post comments, decide how far back to pull posts, and how many posts to pull comments from.\n",
    "## We will be comparing two subreddits, so think of a subject where a comparison might be interesting (e.g. if there are two sides to an issue which may show up in the sentiment analysis as positive and negative scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_posts = 200\n",
    "time_period = 'all'  # use posts from all time\n",
    "\n",
    "# .top() can use the time_period argument\n",
    "# subreddit = reddit.subreddit('').top(time_filter=time_period, limit=number_of_posts)\n",
    "\n",
    "subreddit = reddit.subreddit('').hot(limit=number_of_posts)\n",
    "\n",
    "# Create an empty list to store the data\n",
    "subreddit_comments = []\n",
    "\n",
    "# go through each post in our subreddit and put the comment body and id in our dictionary\n",
    "# the value for 'total' here needs to match 'limit' in reddit.subreddit().top()\n",
    "for post in tqdm(subreddit, total=number_of_posts):\n",
    "    submission = reddit.submission(id=post)\n",
    "    submission.comments.replace_more(limit=0)  # This line of code expands the comments if “load more comments” and “continue this thread” links are encountered\n",
    "    for top_level_comment in submission.comments: \n",
    "        subreddit_comments.append(top_level_comment.body)  # add the comment to our list of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the comments.\n",
    "subreddit_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store comments in a DataFrame using a dictionary as our input\n",
    "# This sets the column name as the key of the dictionary, and the list of values as the values in the DataFrame\n",
    "subreddit_comments_df = pd.DataFrame(data={'comment': subreddit_comments})\n",
    "subreddit_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of how we split up the comments into individual words.\n",
    "# This technique will be used again to get the scores of each individual word.\n",
    "for comment in subreddit_comments_df['comment']:  # loop over each word\n",
    "        comment_words = comment.split()  # split comments into individual words\n",
    "        for word in comment_words:  # loop over idndividual words in each comment\n",
    "            word = word.strip('?:!.,;\"!@()#-')  # remove extraneous characters\n",
    "            word = word.replace(\"\\n\", \"\")  # remove end of line\n",
    "            print(word)\n",
    "        \n",
    "        break  # end the loop after one comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will use the sentiment file called AFINN-en-165.txt.  This file contains a sentiment score for 3382 words.  More information can be found here: https://github.com/fnielsen/afinn With the sentiment file we will assign scores to words within the top comments that are found in the AFINN file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the AFINN sentiment table into a Python dictionary\n",
    "\n",
    "sentimentfile = open(\"AFINN-en-165.txt\", \"r\")  # open sentiment file\n",
    "scores = {}  # an empty dictionary\n",
    "for line in sentimentfile:  # loop over each word / sentiment score\n",
    "    word, score = line.split(\"\\t\")  # file is tab-delimited\n",
    "    scores[word] = int(score)  # convert the scores to intergers\n",
    "    \n",
    "sentimentfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first 10 entries of the dictionary\n",
    "counter = 0\n",
    "for key, value in scores.items():\n",
    "    print(key, ':', value)\n",
    "    counter += 1\n",
    "    if counter >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a dictionary for storing overall counts of sentiment values\n",
    "sentiments = {\"-5\": 0, \"-4\": 0, \"-3\": 0, \"-2\": 0, \"-1\": 0, \"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0}\n",
    "\n",
    "for word in subreddit_comments_df['comment']:  # loop over each word\n",
    "        comment_words = word.split()  # split comments into individual words\n",
    "        for word in comment_words:  # loop over individual words in each comment\n",
    "            word = word.strip('?:!.,;\"!@()#-')  # remove extraneous characters\n",
    "            word = word.replace(\"\\n\", \"\")  # remove end of line\n",
    "            if word in scores.keys():  # check if word is in sentiment dictionary\n",
    "                score = scores[word]  # check if word is in sentiment dictionary\n",
    "                sentiments[str(score)] += 1  # add one to the sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the scores\n",
    "for sentiment_value in range(-5, 6):\n",
    "    # this uses string formatting, more on this here: https://realpython.com/python-f-strings/\n",
    "    print(f\"{sentiment_value} sentiment:\", sentiments[str(sentiment_value)])\n",
    "    \n",
    "# this would be equivalent, but obviously much less compact and elegant\n",
    "# print(\"-5 sentiments \", sentiments[\"-5\"])\n",
    "# print(\"-4 sentiments \", sentiments[\"-4\"])\n",
    "# print(\"-3 sentiments \", sentiments[\"-3\"])\n",
    "# print(\"-2 sentiments \", sentiments[\"-2\"])\n",
    "# print(\"-1 sentiments \", sentiments[\"-1\"])\n",
    "# print(\" 0 sentiments \", sentiments[\"0\"])\n",
    "# print(\" 1 sentiments \", sentiments[\"1\"])\n",
    "# print(\" 2 sentiments \", sentiments[\"2\"])\n",
    "# print(\" 3 sentiments \", sentiments[\"3\"])\n",
    "# print(\" 4 sentiments \", sentiments[\"4\"])\n",
    "# print(\" 5 sentiments \", sentiments[\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us put the sentiment scores into a dataframe.\n",
    "comment_sentiment_df = pd.DataFrame(data={'Sentiment_Value': list(sentiments.keys()), 'Counts': list(sentiments.values())})\n",
    "# the 'value' column is a string; convert to integer (numeric type)\n",
    "comment_sentiment_df['Sentiment_Value'] = comment_sentiment_df['Sentiment_Value'].astype('int')\n",
    "\n",
    "# We normalize the counts so we will be able to compare between two subreddits on the same plot easily\n",
    "comment_sentiment_df['Normalized_Counts'] = comment_sentiment_df['Counts'] / comment_sentiment_df['Counts'].sum()  # Normalize the Count\n",
    "comment_sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "## We will plot the data so it is easier to visualize.  \n",
    "## In each of the three cells below, plot the Count, Normalized Count, and Normalized Score vs Sentiment Value.  In each plot add the appropriate x-label, y-label, plot title, and color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vs Sentiment Value Plot\n",
    "plt.bar(comment_sentiment_df['Sentiment_Value'], comment_sentiment_df[''], color='')  # add the y-values and color\n",
    "plt.xlabel('')  # add x-label\n",
    "plt.ylabel('')  # add y-label\n",
    "plt.title('')  # add title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Counts vs Sentiment Value Plot\n",
    "plt.bar(comment_sentiment_df['Sentiment_Value'], comment_sentiment_df[''], color='')  # add the y-values and color\n",
    "plt.xlabel('')  # add x-label\n",
    "plt.ylabel('')  # add y-label\n",
    "plt.title('')  # add title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "### In the cell below, enter a subreddit you which to compare the sentiment of the post comments, decide how far back to pull posts, and how many posts to pull comments from.\n",
    "\n",
    "Pick a subreddit that can be compared with your first subreddit in terms of sentiment. You may want to go back up to the first subreddit section and change some parameters.  For example, do you want to find top posts, or hot posts? From what time period? How many posts?  If you change these settings above (the `number_of_posts` and `time_period` variables) you should re-run the notebook from the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is the same as we did for our first subreddit, just condensed into one code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_2 = reddit.subreddit('').hot(limit=number_of_posts)\n",
    "\n",
    "\n",
    "# Create an empty list to store the data\n",
    "subreddit_comments_2 = []\n",
    "\n",
    "# go through each post in our subreddit and put the comment body and id in our dictionary\n",
    "for post in tqdm(subreddit_2, total=number_of_posts):\n",
    "    submission = reddit.submission(id=post)\n",
    "    submission.comments.replace_more(limit=0)  # This line of code expands the comments if “load more comments” and “continue this thread” links are encountered\n",
    "    for top_level_comment in submission.comments: \n",
    "        subreddit_comments_2.append(top_level_comment.body)  # add the comment to our list of comments\n",
    "        \n",
    "\n",
    "# Store comments in a DataFrame using a dictionary as our input\n",
    "# This sets the column name as the key of the dictionary, and the list of values as the values in the DataFrame\n",
    "subreddit_comments_df_2 = pd.DataFrame(data={'comment': subreddit_comments_2})\n",
    "    \n",
    "# we create a dictionary for storing overall counts of sentiment values\n",
    "sentiments_2 = {\"-5\": 0, \"-4\": 0, \"-3\": 0, \"-2\": 0, \"-1\": 0, \"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0}\n",
    "\n",
    "for comment in subreddit_comments_df_2['comment']:  # loop over each comment\n",
    "        comment_words = comment.split()  # split comments into individual words\n",
    "        for word in comment_words:  # loop over individual words in each comment\n",
    "            word = word.strip('?:!.,;\"!@()#-')  # remove extraneous characters\n",
    "            word = word.replace(\"\\n\", \"\")  # remove end of line\n",
    "            if word in scores.keys():  # check if word is in sentiment dictionary\n",
    "                score = scores[word]  # check if word is in sentiment dictionary\n",
    "                sentiments_2[str(score)] += 1  # add one to the sentiment score\n",
    "                \n",
    "# Now let us put the sentiment scores into a dataframe.\n",
    "comment_sentiment_df_2 = pd.DataFrame(data={'Sentiment_Value': list(sentiments_2.keys()), 'Counts': list(sentiments_2.values())})\n",
    "# the 'value' column is a string; convert to integer (numeric type)\n",
    "comment_sentiment_df_2['Sentiment_Value'] = comment_sentiment_df_2['Sentiment_Value'].astype('int')\n",
    "\n",
    "# We normalize the counts so we will be able to compare between two subreddits on the same plot easily\n",
    "comment_sentiment_df_2['Normalized_Counts'] = comment_sentiment_df_2['Counts'] / comment_sentiment_df_2['Counts'].sum()  # Normalize the Count\n",
    "comment_sentiment_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "## We will plot the data so it is easier to visualize.  \n",
    "## In each of the three cells below, plot the Count, Normalized Count, and Normalized Score data vs Sentiment Value.  In each plot add the appropriate x-label, y-label, plot title , and color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vs Sentiment Value Plot\n",
    "plt.bar(comment_sentiment_df_2['Sentiment_Value'], comment_sentiment_df_2[''], color='')  # add the y-values and color\n",
    "plt.xlabel('')  # add x-label\n",
    "plt.ylabel('')  # add y-label\n",
    "plt.title('')  # add title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Counts vs Sentiment Value Plot\n",
    "plt.bar(comment_sentiment_df_2['Sentiment_Value'], comment_sentiment_df_2[''], color='')  # add the y-values and color\n",
    "plt.xlabel('')  # add x-label\n",
    "plt.ylabel('')  # add y-label\n",
    "plt.title('')  # add title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "## Now we will overlay the baseline comment sentiment and the subreddit comment sentiment to help compare.\n",
    "\n",
    "## In each of the three cells below, overlay the plots the Count, Normalized Count, and Normalized Score data vs Sentiment Value. In each plot add the appropriate x-label, y-label, plot title, and plot color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vs Sentiment Value Plot\n",
    "plt.bar(comment_sentiment_df['Sentiment_Value'], comment_sentiment_df[''], color='', label='')  # add first subreddit data and color\n",
    "\n",
    "# add second subreddit with a slight offset of x-axis; alpha is opacity/transparency\n",
    "plt.bar(comment_sentiment_df_2['Sentiment_Value'] + 0.2, comment_sentiment_df_2[''], color='', label='', alpha=0.5)  # add second subreddit and color\n",
    "plt.legend()  # show the legend\n",
    "\n",
    "plt.xlabel('')  # add x-label\n",
    "plt.ylabel('')  # add y-label\n",
    "plt.title('')  # add title\n",
    "plt.tight_layout()  # tight_layout() automatically adjusts margins to make it look nice\n",
    "plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Count vs Sentiment Value Plot\n",
    "plt.bar(comment_sentiment_df['Sentiment_Value'], comment_sentiment_df[''], color='', label='')  # add first subreddit data and color\n",
    "ax = plt.gca()  # gets current axes of the plot for adding another dataset to the plot\n",
    "\n",
    "# add second subreddit with a slight offset of x-axis\n",
    "plt.bar(comment_sentiment_df_2['Sentiment_Value'] + 0.2, comment_sentiment_df_2[''], color='', label='', alpha=0.5)  # add second subreddit and color\n",
    "plt.legend()  # show the legend\n",
    "\n",
    "plt.xlabel('')  # add x-label\n",
    "plt.ylabel('')  # add y-label\n",
    "plt.title('')  # add title\n",
    "plt.tight_layout()  # tight_layout() automatically adjusts margins to make it look nice\n",
    "plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stretch goal (bonus-ish)\n",
    "### Although this is not formally a bonus for points, it is a learning opportinity.  You are not required to complete the following part of this notebook for the assignment.\n",
    "\n",
    "Our sentiment analysis technique above works, but has some shortcomings.  The biggest shortcoming is that each word is treated individually.  But what if we have a sentence with a negation?  For example:\n",
    "\n",
    "'This is not a bad thing.'\n",
    "\n",
    "This sentence should be positive overall, but AFINN only has the word 'bad' in the dictionary, and so the sentence gets an overall negative score of -3.\n",
    "\n",
    "The most accurate sentiment analysis methods use neural networks to capture context as well as semantics.  The drawback of NNs is they are computationally expensive to train and run.\n",
    "\n",
    "An easier method is to use a slightly-improved sentiment analysis technique, such as TextBlob or VADER (https://github.com/cjhutto/vaderSentiment) in Python.  Both libraries use a hand-coded algorithm with word scores like AFINN, but also with additions like negation rules (e.g. a word after 'not' has it's score reversed).\n",
    "\n",
    "Other sentiment analysis libraries in Python can be read about here: https://www.iflexion.com/blog/sentiment-analysis-python\n",
    "\n",
    "### The stretch goal\n",
    "The stretch goal is to use other sentiment analysis libraries on the Reddit data we collected, and compare the various approaches (AFINN word-by-word, TextBlob, and VADER) using plots and statistics.  For the AFINN word-by-word approach, you will need to either sum up the sentiment scores for each comment, or average them.  You might also divide them by 5 to get the values between -1 and +1.\n",
    "\n",
    "Here is a brief example of getting scores from the 3 methods described above.  We can see while the raw AFINN approach gives a score of -0.6 (if normalized), TextBlob shows 0.35 and VADER shows 0.43."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'This is not a bad thing.'\n",
    "[(word, scores[word]) for word in sentence.split() if word in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "tb = TextBlob(sentence)\n",
    "print(tb.polarity)\n",
    "print(tb.sentiment_assessments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "analyzer.polarity_scores(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
